{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Report Continuous Control\n",
    "## Environment description\n",
    "The [Unity ML-Agents toolkit](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md) contains the [Reacher Environment](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher) in which double-jointed arms have to move to target locations. I chose the environment with 20 reachers instead of a single one because DRL training methods can benefit from agents sharing their experiences.\n",
    "\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: https://video.udacity-data.com/topher/2018/June/5b1ea778_reacher/reacher.gif \"Trained Agents\"\n",
    "\n",
    "![image1]\n",
    "\n",
    "This environment rewards a reacher in the target location with +0.1. The vector observation space per reacher is 33 dimensional and the vector action space has 4 dimensions corresponding to the torque applicable to two joints. You can verify these numbers in the next cell output.\n",
    "\n",
    "The environment is considered 'solved' when an average reward of +30 per episode is achieved over all 20 reachers and 100 episodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher_Linux/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach: Deep Deterministic Policy Gradient method\n",
    "\n",
    "Due to the continuous action space, classical value-based methods are hard to apply. Instead, we solve the environment using the Deep Deterministic Policy Gradient (DDPG) method. This is a flavour of an actor-critic method where the actor is trained to maximize the expected reward as is predicted by the critic network. The 20 different robotic arms share their experiences and thus learn faster than a single agent.\n",
    "\n",
    "### Implementation\n",
    "I worked on this project as part of the [Deep Reinforcement Learning Udacity Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) and adapted the DDPG agent code from the [Pendulum project in Udacity's Deep Reinforcement Learning Repository](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum) making the following choices and -- sometimes -- changes:\n",
    "\n",
    "#### Ornstein-Uhlenbeck noise\n",
    "\n",
    "In the original code, the noise is generated using this code here\n",
    "\n",
    "```\n",
    "dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "```\n",
    "However, `random.random()` generates uniformly distributed random numbers with mean 0.5. I felt I should change it to standard normally distributed values as is customary for Ornstein-Uhlenbeck by replacing `random.random()` by `np.random.randn()`. I also reduced the noise level by reducing the `sigma` parameter from `0.2` to `0.1`.\n",
    "\n",
    "#### Shared experience buffer\n",
    "There are at least two ways how multiple agents can learn from each other: By sharing\n",
    "- experiences or\n",
    "- network weights.\n",
    "\n",
    "While the latter is the case for popular actor-critic algorithms like A2C or A3C, I decided to first try sharing the experience replay buffer. To that end, it is instantiated once and then passed to the constructors of the agents when instantiating the agents. Please note that we don't have true computational parallelism when training, so race conditions/thread safeness is not an issue here.\n",
    "\n",
    "#### Gradient clipping in the critic training\n",
    "In the course materials, it was mentioned that the training of the critic could benefit from gradient clipping, so I introduced the following line of code before performing `self.critic_optimizer.step()`:\n",
    "```\n",
    "torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "```\n",
    "\n",
    "#### Deep Neural Network architecture\n",
    "\n",
    "For each robotic arm, the actor network has an input size equal to the state size, i.e. 33. Then two hidden layers with 400 and 300 nodes follow and the output layer has 4 nodes, which matches the dimensionality of our action size.\n",
    "\n",
    "The critic has a similar architecture except for the output layer, which only has one node that outputs the expected reward.\n",
    "\n",
    "The activation functions are mostly ReLU except for the output layer, where `tanh` is applied in the actor to exhaust the range (-1,1) or no activation function in the critic to allow for any value as result. This is similar to the Pendulum project, where only the input and action sizes differ.\n",
    "\n",
    "Please note that I also tried out the DNN from the [Bipedal project in Udacity's Deep Reinforcement Learning Repository](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-bipedal), where the actor has one hidden layer less and the critic one hidden layer more. The results during training were disappointing, but I decided not to further investigate why.\n",
    "\n",
    "#### Hyperparameters\n",
    "I chose the following hyperparameters:\n",
    "- replay buffer size of 1e6 is 10x larger than for the Pendulum project, but for 20 agents and 1000 time steps per episodes, this allows to memorize 50 full episodes, which sounded enough to me\n",
    "- learning rate for actor and critic of 1e-4. The learning rate of the critic in the Pendulum project is 10x higher, but as the course materials suggested some gradient clipping for the trainng of the critic, I decided to play it safe also for the learning rate\n",
    "- the discount factor of 0.95 is slightly lower than in the Pendulum project (0.99). I think 0.95 is appropriate, because the episodes are quite long and the long term reward shouldn't influence the short term actions too much.\n",
    "- the soft update parameter tau is 1e-3, which means 0.1% of the weights are transferred by each update. This is the same as in the Pendulum project\n",
    "- the minibatch size is 128, also the same as in the Pendulum project\n",
    "- for sharing experiences, I decided to train all agents with 10 minibatches every 20 time steps as was suggested in the course materials\n",
    "\n",
    "Please note that the training does take some time, which means these choices were based less on rigorous experimentation but rather a combination of some experiments and gut feeling.\n",
    "\n",
    "### Training results\n",
    "If you look at the reward plot for the final configuration, you see that the reachers (each with it's own curve) learn very fast:\n",
    "\n",
    "\n",
    "[image2]: scoregraph.png \"Reward plot\"\n",
    "\n",
    "![image2]\n",
    "The environment is solved if we receive an average reward of more than +30 in 100 episodes. The first score (average over all agents) over 30 is already achieved after 9 episodes. Since the requirement was \"average over 100 episodes\" I let the training run for 100 episodes.\n",
    "\n",
    "A [video of the trained agents](https://github.com/hullmann/continuouscontrol-DDPG/blob/master/Twenty_reachers_trained.mp4) is  part of the repository as are the [weights of the agents](https://github.com/hullmann/continuouscontrol-DDPG/tree/master/model_weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas for future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was fascinated by the effect of parallel training and experience sharing.\n",
    "I think it would be great to extend the source code such that\n",
    "- the agents run and train as separated processes (\"embarrassingly parallel\") such that the runtime can be kept low even if we have 20 agents in parallel\n",
    "- also the network weights and not only the experience replay buffer are shared among the agents and to evaluate the -- hopefully positive -- effect of this exchange"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
